---
title: "Home"
metaTitle: "Neural Magic Documentation"
metaDescription: "Documentation for Neural Magic"
index: 0
---

# Neural Magic Documentation

Welcome to software-delivered AI... Welcome to Neural Magic! We enable you to deploy deep learning models on commodity CPUs with GPU-class performance.

Through our guided experiences and documentation, you can discover how to unlock the full potential of your ML environment and accommodate the continuous growth of neural networks without adding complexity or cost.

Neural Magic guided experiences and documentation will show how you can integrate ML into your application.
You can do so quickly using DeepSparse, which is an inference runtime offering GPU class performance on CPUs and APIs for the fastest AI performance.
First, select a model to deploy, which can be your own model in ONNX format or a model from Neural Magic’s SparseZoo&mdash;an open-source repository for sparse and sparse-quantized models.
Then, use DeepSparse to benchmark, evaluate, and inference.

<img src="/src/images/infographic-extract1.png" />

Alternatively, you can sparsify your selected model before deploying to DeepSparse.
Sparsification is accomplished with Neural Magic’s Sparsify&mdash;an ML optimization product to accelerate inference at scale&mdash;or SparseML&mdash;an open-source sparsification research framework.
Once your model is optimized, use DeepSparse to benchmark, evaluate, and inference.
Because DeepSparse reaches GPU-class performance with commodity CPUs, you do not need to tether deployments to accelerators in order to reach the performance needed for production.

<img src="/src/images/infographic-extract2.png" />

In summary, Neural Magic’s product suite enables two major workflows.

1. **Deploying a Model on CPUs**

    DeepSparse is an inference runtime offering GPU class performance on CPUs and APIs for integrating ML into an application. When running an inference-optimized sparse model, DeepSparse on commodity CPUs achieves better latency than a NVIDIA T4 (the most common GPU for inference) and an order of magnitude more throughput than ONNX runtime. As a result, DeepSparse offers the best price-performance for deep learning deployments.

    DeepSparse is not only fast and CPU-only, but is also easy to add to your application. With DeepSparse, you can spend less time writing scaffolding-code and focus more on building a great system.

2. **Optimizing a Model for Inference**

    SparseZoo and SparseML work together to optimize models for inference with sparsity techniques, such as pruning and quantization.

    SparseZoo is an open-source repository of pre-sparsified models (for example, sparse ResNet-50 has 95% of weights set to 0 while maintaining 99% of the baseline accuracy).
    SparseZoo is integrated with SparseML, making it simple for you to sparse transfer learning (fine-tune from a sparse model) onto your data.

    SparseML is an open-source library that extends PyTorch and TensorFlow to simplify the process of applying sparsity algorithms.
    Via simple CLI scripts or five lines of code, you can sparsify any model from scratch, or sparse transfer learn from pre-sparsified versions of foundation models such as ResNet, YOLOv5, or BERT.

<img src="/src/images/infographic.png" />

This was a quick look at Neural Magic’s suite of products. Now, we invite you to explore our products through guided experiences and documentation.

## Next Steps

✅ Explore this site:
- Take a deeper dive into terms and concepts.
- **Get Started** provides a tour of major functionality.
- **Tasks walk** through detailed examples using supported ML use cases.
- **Guides** provide how-to, product, and API-level documents for all classes and functions.
- **Details** include research papers, a glossary, and FAQs.
- **Help** links to the various product support channels.

✅ Jump right in:
- Optimize for inference and apply sparsity to your models.
- Deploy on CPUs and learn about the benefits of deploying on CPUs.
- Take a quick tour as a run through of our capabilities.

✅ Join Neural Magic!
- Join our [community](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ) if you need any help or [subscribe](https://neuralmagic.com/deep-sparse-community/#subscribe) for regular email updates.
- Check out our [GitHub repositories](https://github.com/neuralmagic) and give us a ⭐.
- Help us improve this [documentation](https://github.com/neuralmagic/docs).
