---
title: "Sparsify a Model"
metaTitle: "Sparsify a Model"
metaDescription: "Sparsify a model with SparseML and recipes for smaller, faster, and cheaper model inferences in deployment"
index: 4000
---

# Sparsify a Model

SparseML enables you to create a sparse model from scratch. The library contains state-of-the-art sparsification algorithms, including pruning, distillation, and quantization techniques.

These algorithms are built on top of sparsification recipes, enabling easy integration into custom ML training pipelines to sparsify most neural networks.
Additionally, SparseML integrates with popular ML repositories like Hugging Face Transformers and Ultralytics YOLO. With these integrations, creating a recipe and passing it to a CLI is all you need to sparsify a model.

Aside from sparsification algorithms, SparseML contains generic export pathways for performant deployments.
These export pathways ensure the model saves in the correct format and rewrites the inference graphs for performance, such as quantized operator folding.
The results are simple to export CLIs and APIs that guarantee performance for sparsified models in their given deployment environment.

## Example Use Cases

The documents below walk through use cases, leveraging SparseML to sparsify models with recipes and exporting for performant inference.

<LinkCards>
  <LinkCard href="./supported-integrations" heading="Supported Integrations">
    Example creating a recipe and utilizing it with supported SparseML integrations to create sparsified models.
  </LinkCard>

  <LinkCard href="./custom-integrations" heading="Custom Integrations">
    Example enabling SparseML sparsification techniques with a custom ML pipeline to create sparsified models.
  </LinkCard>
</LinkCards>

## Other Use Cases

More documentation, models, use cases, and examples are continually being added.
If you don't see one you're interested in, search the [DeepSparse GitHub repository](https://github.com/neuralmagic/deepsparse), the [SparseML GitHub repository](https://github.com/neuralmagic/sparseml), the [SparseZoo website](https://sparsezoo.neuralmagic.com/), or ask in the [Neural Magic Slack](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ).
