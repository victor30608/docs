---
title: "Supported Integrations"
metaTitle: "Sparsifying a Model for SparseML Integrations"
metaDescription: "Sparsify a model with SparseML and recipes for smaller, faster, and cheaper model inferences in deployment"
index: 1000
---

# Sparsifying a Model for SparseML Integrations

This page walks through an example of creating a sparsification recipe to prune a dense model from scratch and applying a recipe to a supported integration.

SparseML has pre-made integrations with many popular model repositories, such as with Hugging Face Transformers and Ultralytics YOLOv5.
For these integrations, a sparsification recipe is all you need. You can apply state-of-the-art sparsification algorithms, including
pruning, distillation, and quantization with a single command line call.

## Installation Requirements

This section requires [SparseML Torchvision Installation](/get-started/install/sparseml) to run the Apply the Recipe section.

## Pruning and Pruning Recipes

Pruning is a systematic way of removing redundant weights and connections within a neural network. An applied pruning algorithm must determine which
weights are redundant and will not affect the accuracy.

A standard algorithm for pruning is gradual magnitude pruning (GMP).
With it, the weights closest to zero are iteratively removed over several epochs or training steps. The non-zero weights are then fine-tuned to the objective function.
This iterative process enables the model to adjust to a new optimization space after pathways are removed before pruning again.

Important hyperparameters that need to be set are:
 - Layers to prune and their target sparsity levels
 - Number of epochs for pruning
 - Frequency of pruning
 - Length of time to fine-tune after pruning
 - Learning rates (LR) for pruning and fine-tuning

The proper hyperparameter values will differ for different model architectures, training schemes, and domains; but, there is some general intuition for safe starting values.
The following are reasonable default values with which to start:
  - Final sparsity is set to 80% sparsity applied globally across all layers.
  - Funning frequency is set to pruning once per epoch (up to a few times per epoch for shorter schedules).
  - Number of pruning epochs is set to 1/3 the original training epochs.
  - Number of fine-tuning epochs is set to 1/4 the original epochs.
  - Pruning LR is set to the midrange from the model's training start and final LRs.
  - The fine-tuning LRs cycle (from the pruning LR to the final LR) is used for training.

SparseML conveniently encodes these hyperparameters into a YAML-based **Recipe** file. The rest of the system parses the arguments in the YAML file to set the parameters of the algorithm.

For example, the following `recipe.yaml` file uses the default values listed above:
```yaml
modifiers:
    - !GlobalMagnitudePruningModifier
        init_sparsity: 0.05
        final_sparsity: 0.8
        start_epoch: 0.0
        end_epoch: 30.0
        update_frequency: 1.0
        params: __ALL_PRUNABLE__

    - !SetLearningRateModifier
        start_epoch: 0.0
        learning_rate: 0.05

    - !LearningRateFunctionModifier
        start_epoch: 30.0
        end_epoch: 50.0
        lr_func: cosine
        init_lr: 0.05
        final_lr: 0.001

    - !EpochRangeModifier
        start_epoch: 0.0
        end_epoch: 50.0
```

In this recipe:
  - `GlobalMagnitudePruningModifier` applies gradual magnitude pruning globally across all the prunable parameters/weights in a model.
  - `GlobalMagnitudePruningModifier` starts at 5% sparsity at epoch 0 and gradually ramps up to 80% sparsity at epoch 30, pruning at the start of each epoch.
  - `SetLearningRateModifier` sets the pruning LR to 0.05 (midpoint between the original 0.1 and 0.001 training LRs).
  - `LearningRateFunctionModifier` cycles the fine-tuning LR from the pruning LR to 0.001 with a cosine curve. (0.001 was the final original training LR.)
  - `EpochRangeModifier` expands the training time to continue fine-tuning for an additional 20 epochs after pruning has ended.
  - 30 pruning epochs and 20 fine-tuning epochs were chosen based on a 90 epoch training schedule. Be sure to adjust based on the number of epochs used for the initial training for your use case.

## Quantization and Quantization Recipes

A quantization recipe systematically reduces the precision for weights and activations within a neural network, generally from `FP32` to `INT8`. Running a quantized
model increases speed and reduces memory consumption while sacrificing very little in terms of accuracy.

Quantization-aware training (QAT) is the standard algorithm. With QAT, fake quantization operators are injected into the graph before quantizable nodes for activations, and weights are wrapped with fake quantization operators.
The fake quantization operators interpolate the weights and activations down to INT8 on the forward pass but enable a full update of the weights at FP32 on the backward pass.
The updates to the weights at FP32 throughout the training process allow the model to adapt to the loss of information from quantization on the forward pass.
QAT generally guarantees better recovery for a given model compared with post-training quantization (PTQ), where training is not used.

Important hyperparameters for QAT are the learning rate (LR), the number of epochs to train for while quantized, and determination of when to freeze batch normalization statistics for CNNs.
Freezing batch normalization statistics enables the folding of these operators into convolutions for inference time and is an essential step for QAT.
The proper hyperparameter values will differ for different model architectures, training schemes, and domains, but there is some general intuition for safe starting values.
The following are reasonably good values with which to start:
  - LR is set to 0.1 or 0.01 times the value of the final LR during training.
  - The number of quantized training epochs is set to 5.
  - Batch normalization statistics are frozen at the start of the third epoch.

For example, the following `recipe.yaml` file uses the default values listed above:
```yaml
modifiers:
    - !QuantizationModifier
        start_epoch: 0.0
        freeze_bn_stats_epoch: 3.0

    - !SetLearningRateModifier
        start_epoch: 0.0
        learning_rate: 10e-6

    - !EpochRangeModifier
        start_epoch: 0.0
        end_epoch: 5.0
```

In this recipe:
  - `QuantizationModifier` applies QAT to all quantizable modules under the `model` scope.
Note that the `model` is used here as a general placeholder; to determine the name of the root module for your model, print out the root module and use that root name.
  - `QuantizationModifier` starts at epoch 0 and freezes batch normalization statistics at the start of epoch 3.
  - `SetLearningRateModifier` sets the quantization LR to 10e-6 (0.01 times the example final LR of 0.001).
  - `EpochRangeModifier` sets the training time to continue training for the desired 5 epochs.

## Pruning Plus Quantization Recipe

To create a pruning and quantization recipe, the pruning and quantization recipes are merged from the previous sections.
Quantization is added after pruning and fine-tuning are complete such that the training cycles end with it.
This prevents stability issues from lacking precision when pruning and utilizing larger LRs.

Combining the two previous recipes creates the following new `recipe.yaml` file:
```yaml
modifiers:
    - !GlobalMagnitudePruningModifier
        init_sparsity: 0.05
        final_sparsity: 0.8
        start_epoch: 0.0
        end_epoch: 30.0
        update_frequency: 1.0
        params: __ALL_PRUNABLE__

    - !SetLearningRateModifier
        start_epoch: 0.0
        learning_rate: 0.05

    - !LearningRateFunctionModifier
        start_epoch: 30.0
        end_epoch: 50.0
        lr_func: cosine
        init_lr: 0.05
        final_lr: 0.001

    - !QuantizationModifier
        start_epoch: 50.0
        freeze_bn_stats_epoch: 53.0

    - !SetLearningRateModifier
        start_epoch: 50.0
        learning_rate: 10e-6

    - !EpochRangeModifier
        start_epoch: 0.0
        end_epoch: 55.0
```

## Applying a Recipe

The recipe created can now be applied using the SparseML integrations.
For example, SparseML installs with a CLI utilizing the Ultralytics YOLOv5 repository and training pathways, among others.
To view instructions for the CLI, run the following command:
```bash
sparseml.yolov5.train --help
```

To use the recipe given in the previous section, save it locally as a `recipe.yaml` file.
Next, it can be passed in for the `--recipe` argument in the YOLOv5 train CLI.

By running the following command, you will apply the GMP and QAT algorithms encoded in the recipe to the dense version of YOLOv5s,
which is pulled down from the SparseZoo. In this example, the fine-tuning is done onto the COCO dataset.

```bash
sparseml.yolov5.train \
  --weights zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/base-none \
  --data coco.yaml \
  --hyp data/hyps/hyp.scratch.yaml \
  --recipe recipe.yaml
```
