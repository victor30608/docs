---
title: "Basic Concepts"
metaTitle: "Basic Conccepts"
metaDescription: "Overview of Neural Magic Platform features"
index: 1000
---

# Basic Concepts

<img src="/src/images/infographic-concepts.png">

## Model Selection

When selecting either your own model or one from SparseZoo, it is important to understand sparsification. As such, model selection is described as an integral part of the following discussions.

## Sparsification

The process of sparsification takes a trained deep learning model and removes redundant information from the over-parameterized network, resulting in a faster and smaller model. Techniques for sparsification include everything from inducing sparsity using pruning and quantization (described more below) to distilling from a larger model to create a smaller version. When implemented correctly, these techniques result in significantly more performant and smaller models with limited to no effect on the baseline metrics.

Combining multiple sparsification algorithms will generally result in more compressed and faster models than any individual algorithm. This combination of algorithms is termed **compound sparsification**. For example, combining both pruning and quantization is very common to create sparse-quantized models that can be up to 4 times smaller. Additionally, it is common for NLP models to combine distillation, weight pruning, layer dropping, and quantization to create much smaller models that recover close to the original baseline.

Ultimately the power of sparsification is only realized when the deployment environment supports it. **DeepSparse** is specifically engineered to utilize sparse networks for GPU-class performance on CPUs.

## Model Optimization for Inference

There are multiple factors to consider when creating a deep learning model. In training, accuracy on the test-set is the primary metric. In deployment, the performance (latency/throughput) of the model becomes an important consideration at production scale. However, as deep learning has exploded and state-of-the-art models have grown bigger and bigger, performance and accuracy are increasingly at odds.

### Sparsity

Sparsity improves performance while maintaining high accuracy. SparseZoo and SparseML work together to help you create performance-optimized models while minimizing accuracy loss by using sparsification techniques called pruning and quantization.

### Pruning

Pruning is the process of removing weights from a trained deep learning model by setting them to zero. Pruning can speed up a model because inference runtimes implement optimizations that "skip" the multiply-adds by zero, reducing the needed computation.

Two types of pruning can be applied to a model:

- **Structured pruning**
Weights are pruned in groups (for example, entire channels or nodes). With structured pruning, it is easy for an inference runtime to include optimizations that speed-up the mode. Most runtimes will benefit from this type of pruning. However, structured pruning can have large negative impacts on the accuracy of the model, especially at the high levels of sparsity needed to see speedups.

- **Unstructured pruning**
Weights (or small groups of weights) are pruned in any pattern. With unstructured pruning, it is *very hard* for an inference runtime to include optimizations that speed-up the model. (As far as we know, DeepSparse is the only production-grade runtime focused on speedups from unstructured pruning.) The benefit of unstructured pruning, however, is that sparsity can be pushed to very high levels while maintaining high levels of accuracy. With both CNNs (ResNet-50) and transformers (BERT-base), Neural Magic has pruned 95% of weights while maintaining 99% of the accuracy of the baseline models.

Neural Magicâ€™s blog provides a detailed conceptual discussion of pruning.

### Quantization

Quantization is a technique to reduce computation and memory usage by converting the parameters and activations of a model from a high precision format like FP32 (which is the default for a deep learning model) to a low precision format like INT8.

By using lower precision, runtimes can reduce memory footprint and perform operations (such as matrix multiply) faster. Additionally, quantization can be combined with unstructured pruning to gain additional speedup, a concept we call compound sparsity.

### Training-Aware Algorithms

Broadly, there are two ways that pruning and quantization can be applied to a model:

- **Post-training**
Sparsity is applied in one pass with no training data. Post-training pruning and quantization optimizations are easier to apply to a model. However, these techniques often create significant drops in accuracy, as the model does not have a chance to readjust to the optimization space.

- **Training-aware**
Sparsity is applied gradually, and the non-zero weights are adjusted with training data. Training-aware pruning and quantization require setting up a training pipeline and implementing complex algorithms. However, applying the pruning and quantization gradually and fine-tuning the non-zero weights with training data enables accuracy to recover to 99% of the baseline dense model even as sparsity is pushed to very high levels.

**SparseZoo** and **SparseML** use **training-aware unstructured pruning** and **training-aware quantization** (as well as post-training) to create very sparse models that sacrifice very little accuracy. Training-aware techniques apply the sparsification gradually, allowing the model to adjust by fine-tuning the remaining weights with the training dataset at each step. This technique is critical to maintaining high accuracy at the high levels of sparsity needed to reach GPU-class performance.
